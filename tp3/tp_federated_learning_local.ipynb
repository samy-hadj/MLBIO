{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importation des librairies PyTorch et torchvision\n",
        "\n",
        "Ce script commence par importer les librairies PyTorch et torchvision, qui sont utilisées pour le développement d'applications de machine learning et de vision par ordinateur.\n",
        "\n",
        "- `torch` : Librairie principale pour l’apprentissage automatique et le traitement des tensors.\n",
        "- `torchvision` : Librairie complémentaire à PyTorch contenant des datasets, des transformations, et des modèles pré-entraînés utiles pour les tâches de vision par ordinateur."
      ],
      "metadata": {
        "id": "xVa8khSyIoEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6BtXcJhF-ot"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chargement et préparation du dataset MNIST\n",
        "\n",
        "Ce script charge et prépare le dataset MNIST, un célèbre dataset de digits manuscrits. MNIST est souvent utilisé pour les tâches de classification.\n",
        "\n",
        "### Préparation des transformations\n",
        "\n",
        "Les transformations appliquées au dataset sont composées comme suit :\n",
        "- `transforms.ToTensor()` : Convertit les images en tensors.\n",
        "- `transforms.Normalize((0.5,), (0.5,))` : Normalise les images en les centrant autour de 0.5 avec une échelle de 0.5.\n"
      ],
      "metadata": {
        "id": "sNxGtNF_IwAC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75i3VQUXF-ou"
      },
      "outputs": [],
      "source": [
        "# Définition des transformations pour les données MNIST\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),  # Convertit les images en tensors PyTorch\n",
        "     transforms.Normalize((0.5,), (0.5,))]  # Normalise les données pour les centrer autour de 0 et normalise à l’échelle de 0.5\n",
        ")\n",
        "\n",
        "# Chargement de l'ensemble de données MNIST pour l'entraînement\n",
        "sub_mnist_dataset1 = datasets.MNIST(\n",
        "    \"data\",  # Chemin où les données seront stockées/téléchargées\n",
        "    train=True,  # Utilise les données d'entraînement\n",
        "    download=True,  # Télécharge les données si elles ne sont pas encore présentes\n",
        "    transform=transform  # Applique les transformations définies plus haut\n",
        ")\n",
        "\n",
        "# Extraction d'un sous-ensemble des données : indices allant de 1200 à 1800 (données d'entraînement)\n",
        "sub_mnist_dataset1.data = sub_mnist_dataset1.data[1200:1800]  # Sélection des lignes entre 1200 et 1800\n",
        "sub_mnist_dataset1.target = sub_mnist_dataset1.targets[1200:1800]  # Sélection des targets correspondants\n",
        "\n",
        "# Chargement du deuxième sous-ensemble de l'ensemble de données MNIST pour l'entraînement\n",
        "sub_mnist_dataset2 = datasets.MNIST(\n",
        "    \"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Extraction d'un sous-ensemble des données : indices allant de 600 à 1200 (données d'entraînement)\n",
        "sub_mnist_dataset2.data = sub_mnist_dataset2.data[600:1200]  # Sélection des lignes entre 600 et 1200\n",
        "sub_mnist_dataset2.target = sub_mnist_dataset2.targets[600:1200]  # Sélection des targets correspondants\n",
        "\n",
        "# Chargement des données de test de l'ensemble MNIST\n",
        "sub_mnist_dataset_test = datasets.MNIST(\n",
        "    \"data\",\n",
        "    train=False,  # Utilise les données de test\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Chargement des données d'entraînement avec DataLoader pour le premier sous-ensemble\n",
        "train_loader1 = torch.utils.data.DataLoader(\n",
        "    sub_mnist_dataset1,  # Premier sous-ensemble d'entraînement\n",
        "    batch_size=50,  # Taille des mini-batchs pour l'entraînement\n",
        "    shuffle=True  # Mélange des données à chaque époque\n",
        ")\n",
        "\n",
        "# Chargement des données d'entraînement avec DataLoader pour le second sous-ensemble\n",
        "train_loader2 = torch.utils.data.DataLoader(\n",
        "    sub_mnist_dataset2,  # Second sous-ensemble d'entraînement\n",
        "    batch_size=50,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Chargement des données de test avec DataLoader\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    sub_mnist_dataset_test,  # Données de test\n",
        "    batch_size=50,\n",
        "    shuffle=True  # Mélange des données à chaque époque\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Définition du modèle Neural Network\n",
        "\n",
        "Ce script définit un réseau de neurones convolutif simple pour la classification d'images.\n",
        "\n",
        "### Classe `Net`\n",
        "La classe `Net` hérite de `nn.Module`, ce qui lui permet d'implémenter les différentes couches du modèle.\n",
        "\n",
        "- **Conv1** : Première couche de convolution avec 1 canal d'entrée et 4 canaux de sortie, une taille de filtre de 3x3 et un pas de 1.\n",
        "- **Conv2** : Deuxième couche de convolution avec 4 canaux d'entrée et 2 canaux de sortie, également avec une taille de filtre de 3x3 et un pas de 1.\n",
        "- **fc1** : Première couche entièrement connectée (FC) prenant les 50 valeurs de sortie de la dernière couche convolutive et les transformant en 32 neurones.\n",
        "- **fc2** : Deuxième couche entièrement connectée, prenant les 32 neurones de la couche précédente et les transformant en 10 classes (output final).\n",
        "\n",
        "### Fonction `forward`\n",
        "La fonction `forward` est responsable de la propagation des données à travers le réseau :\n",
        "- **conv1** : Applique une convolution suivie d’une activation ReLU.\n",
        "- **max_pool2d** : Applique un max-pooling avec une taille de 2x2.\n",
        "- **conv2** : Applique une deuxième convolution suivie d’une activation ReLU.\n",
        "- **max_pool2d** : Applique un deuxième max-pooling avec une taille de 2x2.\n",
        "- **flatten** : Met les données convolutives en un vecteur plat de taille appropriée.\n",
        "- **fc1** : Applique une couche fully connectée avec une activation ReLU.\n",
        "- **fc2** : Applique la couche fully connectée finale.\n",
        "- **log_softmax** : Applique une fonction log-softmax pour obtenir les probabilités des différentes classes."
      ],
      "metadata": {
        "id": "-v8nKfwsI2Dv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJyZY4efF-ou"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn  # Importation du module pour les couches de réseaux neuronaux de PyTorch\n",
        "import torch.nn.functional as F  # Importation des fonctions pour les calculs dans les couches\n",
        "\n",
        "# Définition du modèle de réseau neuronal (class Net) qui hérite de nn.Module\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, seed=0):\n",
        "        super(Net, self).__init__()  # Appel du constructeur de la classe parent nn.Module\n",
        "\n",
        "        torch.manual_seed(seed)  # Fixation du seed pour reproductibilité des résultats\n",
        "\n",
        "        # Première couche convolutionnelle : 1 entrée, 4 sorties, taille du noyau 3x3\n",
        "        self.conv1 = nn.Conv2d(1, 4, 3, 1)\n",
        "\n",
        "        # Deuxième couche convolutionnelle : 4 entrées, 2 sorties, taille du noyau 3x3\n",
        "        self.conv2 = nn.Conv2d(4, 2, 3, 1)\n",
        "\n",
        "        # Première couche de connexion linéaire (fully connected) : 50 entrées, 32 sorties\n",
        "        self.fc1 = nn.Linear(50, 32)\n",
        "\n",
        "        # Deuxième couche de connexion linéaire (fully connected) : 32 entrées, 10 sorties\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "\n",
        "    # Fonction de passage en avant (forward pass)\n",
        "    def forward(self, x):\n",
        "        # Application de la première couche convolutionnelle suivie d'une activation ReLU\n",
        "        x = F.relu(self.conv1(x))\n",
        "\n",
        "        # Application d'une opération de max pooling sur les cartes de convolution\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "        # Application de la deuxième couche convolutionnelle suivie d'une activation ReLU\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # Application d'une autre opération de max pooling\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "        # Ré-organisation des dimensions pour passer à la couche fully connected\n",
        "        x = x.view(x.size(0), -1)  # Flattening des dimensions des feature maps\n",
        "\n",
        "        # Passage à la première couche fully connected avec activation ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Passage à la deuxième couche fully connected sans activation car la sortie est utilisée en softmax\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Softmax pour normaliser les sorties au niveau des probabilités\n",
        "        return F.log_softmax(x, dim=1)  # Retourne les probabilités logaires\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fonctions d'agrégation et de mise à jour des paramètres du modèle\n",
        "\n",
        "## 1. `average_model_parameters(models, average_weights)`\n",
        "Cette fonction prend en entrée :\n",
        "- **models** : Une liste de modèles PyTorch (`nn.Module`) dont on souhaite obtenir les paramètres.\n",
        "- **average_weights** : Un tenseur contenant les poids de chaque modèle, utilisés pour pondérer les contributions des différents modèles.\n",
        "\n",
        "Elle retourne :\n",
        "- **avg_params** : Une liste contenant les paramètres moyens des modèles, pondérés par `average_weights`.\n",
        "\n",
        "### Description :\n",
        "- **avg_params** : Initialise une liste de zéro de la même taille que les paramètres du premier modèle.\n",
        "- **Boucle** : Pour chaque modèle dans la liste, elle ajoute les paramètres pondérés à la liste `avg_params`.\n",
        "- **Sortie** : Une liste contenant les paramètres moyennés de tous les modèles.\n"
      ],
      "metadata": {
        "id": "rNy2hCO3I8HD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZKe2gq1bI8Ft"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1g-2l2gF-ou"
      },
      "outputs": [],
      "source": [
        "# Fonction pour calculer les paramètres moyens à partir de plusieurs modèles\n",
        "def average_model_parameters(models: list[nn.Module], average_weights: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Calcule les paramètres moyens des modèles fournis, en pondérant les paramètres selon des poids spécifiques.\n",
        "\n",
        "    Args:\n",
        "        models (list[nn.Module]): Liste de modèles PyTorch.\n",
        "        average_weights (torch.Tensor): Tenseur contenant les poids associés à chaque modèle.\n",
        "\n",
        "    Returns:\n",
        "        list[torch.Tensor]: Liste des paramètres moyens résultants.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialisation d'une liste pour stocker les paramètres moyens\n",
        "    avg_params = [torch.zeros_like(p) for p in models[0].parameters()]\n",
        "\n",
        "    # Boucle sur chaque modèle dans la liste\n",
        "    for nb_model, model in enumerate(models):\n",
        "        # Boucle sur chaque paramètre du modèle\n",
        "        for i, p in enumerate(model.parameters()):\n",
        "            # Accumule les paramètres pondérés en fonction des poids fournis\n",
        "            avg_params[i] += p * average_weights[nb_model]\n",
        "\n",
        "    return avg_params\n",
        "\n",
        "\n",
        "# Fonction pour mettre à jour les paramètres d'un modèle avec les paramètres moyens\n",
        "def update_model(model, avg_params):\n",
        "    \"\"\"\n",
        "    Met à jour les paramètres d'un modèle avec les paramètres moyens donnés.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Modèle PyTorch dont les paramètres vont être mis à jour.\n",
        "        avg_params (list[torch.Tensor]): Liste des paramètres moyens à utiliser pour la mise à jour.\n",
        "    \"\"\"\n",
        "\n",
        "    # Boucle sur chaque paramètre du modèle\n",
        "    for i, p in enumerate(model.parameters()):\n",
        "        # Met à jour les paramètres du modèle avec les moyennes calculées\n",
        "        p.data = avg_params[i]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonction `train_batch(model, data, target, optimizer)`\n",
        "Cette fonction effectue une étape de formation sur un batch spécifique de données.\n",
        "\n",
        "### Description :\n",
        "- **model.train()** : Met le modèle en mode d'entraînement.\n",
        "- **optimizer.zero_grad()** : Met à zéro les gradients des paramètres du modèle.\n",
        "- **output** : Calcule la sortie du modèle pour les données.\n",
        "- **loss** : Calcul du loss avec la fonction `nll_loss` (Negative Log Likelihood Loss).\n",
        "- **loss.backward()** : Calcule les gradients des paramètres en fonction de la perte.\n",
        "- **optimizer.step()** : Applique les mises à jour des paramètres du modèle selon les gradients.\n",
        "- **Retour** : Le modèle entraîné après la formation d’un batch."
      ],
      "metadata": {
        "id": "F6MCyBdDJCGB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4ktAEG2F-ov"
      },
      "outputs": [],
      "source": [
        "# Optimiseur Adam pour l'entraînement du modèle\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "def train_batch(model, data, target, optimizer):\n",
        "    \"\"\"\n",
        "    Entraîne le modèle sur un batch de données.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): Le modèle PyTorch à entraîner.\n",
        "        data (torch.Tensor): Les données d'entrée.\n",
        "        target (torch.Tensor): Les cibles/étiquettes.\n",
        "        optimizer (torch.optim.Optimizer): L'optimiseur utilisé pour la descente de gradient.\n",
        "\n",
        "    Returns:\n",
        "        nn.Module: Le modèle entraîné.\n",
        "    \"\"\"\n",
        "    model.train()  # Passe le modèle en mode d'entraînement\n",
        "    optimizer.zero_grad()  # Met les gradients à zéro\n",
        "    output = model(data)  # Passer les données à travers le modèle\n",
        "    loss = F.nll_loss(output, target)  # Calcule la perte\n",
        "    loss.backward()  # Effectue la rétropropagation pour calculer les gradients\n",
        "    optimizer.step()  # Met à jour les paramètres du modèle\n",
        "    return model  # Retourne le modèle entraîné\n",
        "\n",
        "def train(models, train_loaders, average_weights, epochs=5, rounds=2):\n",
        "    \"\"\"\n",
        "    Entraîne plusieurs modèles sur leurs sous-ensembles de données.\n",
        "\n",
        "    Args:\n",
        "        models (list[nn.Module]): Liste des modèles PyTorch à entraîner.\n",
        "        train_loaders (list[DataLoader]): Liste de chargeurs de données pour chaque modèle.\n",
        "        average_weights (torch.Tensor): Poids pour les modèles lors de l'average des paramètres.\n",
        "        epochs (int): Nombre d'époques pour l'entraînement.\n",
        "        rounds (int): Nombre de tours d'entraînement.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialisation des optimiseurs pour chaque modèle\n",
        "    optimizers = [torch.optim.SGD(model.parameters(), lr=1e-1) for model in models]\n",
        "\n",
        "    for round in range(rounds):  # Parcourt chaque tour d'entraînement\n",
        "        epoch_losses = [[] for _ in range(len(models))]  # Stocke les pertes pour chaque modèle\n",
        "\n",
        "        for epoch in range(epochs):  # Parcourt chaque époque\n",
        "            for i, loader in enumerate(train_loaders):  # Parcourt les chargeurs de données\n",
        "                for data, target in loader:\n",
        "                    model = models[i]  # Modèle correspondant au chargeur\n",
        "                    optimizer = optimizers[i]  # Optimiseur correspondant au modèle\n",
        "\n",
        "                    model.train()  # Passe le modèle en mode d'entraînement\n",
        "                    optimizer.zero_grad()  # Met les gradients à zéro\n",
        "\n",
        "                    output = model(data)  # Passe les données à travers le modèle\n",
        "                    loss = F.nll_loss(output, target)  # Calcule la perte\n",
        "                    loss.backward()  # Effectue la rétropropagation pour calculer les gradients\n",
        "                    optimizer.step()  # Met à jour les paramètres du modèle\n",
        "\n",
        "                    epoch_losses[i].append(loss.item())  # Stocke la perte pour cette itération\n",
        "\n",
        "        # Calcul des paramètres moyens à la fin de chaque tour\n",
        "        avg_params = average_model_parameters(models, average_weights)\n",
        "\n",
        "        for i, model in enumerate(models):  # Affiche et met à jour chaque modèle après le round\n",
        "            avg_loss = sum(epoch_losses[i]) / len(epoch_losses[i])\n",
        "            print(f\"Model {i}, Round {round}, Loss: {avg_loss}\")\n",
        "            update_model(model, avg_params)  # Met à jour les paramètres du modèle avec l'average\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seed aléatoire\n",
        "\n",
        "### Configuration des hyperparamètres\n",
        "- **`epochs`** : Définie le nombre d’époques à effectuer pour chaque round d’entraînement.\n",
        "- **`rounds`** : Nombre total de rounds au cours desquels la formation est effectuée.\n"
      ],
      "metadata": {
        "id": "HxPikZI2JV8K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cw3WW1yF-ow"
      },
      "outputs": [],
      "source": [
        "rounds = 50\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialisation des modèles et chargeurs de données\n",
        "\n",
        "### model1 et model2 : Création de deux instances du modèle Net, qui représente un réseau de neurones convolutif simple.\n",
        "### models : Liste contenant les deux modèles à entraîner.\n",
        "### train_loaders : Liste des DataLoader, contenant les sous-ensembles d’entraînement pour chaque batch.\n",
        "### average_weights : Poids pour l’agrégation des paramètres, dans cet exemple, ils sont équilibrés à 0.5 pour chaque modèle."
      ],
      "metadata": {
        "id": "Uh8Jpiy_JatW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ47UAUBF-ow"
      },
      "outputs": [],
      "source": [
        "# Définition de deux modèles identiques\n",
        "model1 = Net()  # Instanciation du premier modèle basé sur la classe Net\n",
        "model2 = Net()  # Instanciation du deuxième modèle basé sur la classe Net\n",
        "\n",
        "# Stockage des modèles dans une liste pour la gestion simultanée\n",
        "models = [model1, model2]  # Liste des modèles PyTorch à entraîner\n",
        "\n",
        "# Liste des chargeurs de données correspondant à chaque modèle (train_loader1, train_loader2)\n",
        "train_loaders = [train_loader1, train_loader2]\n",
        "\n",
        "# Poids associés à chaque modèle pour l'average des paramètres\n",
        "average_weights = torch.tensor([0.5, 0.5])  # Poids uniformes pour les deux modèles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exécution de la formation\n",
        "## train() : Fonction principale pour l’entraînement des modèles. Elle prend en entrée les modèles, les chargeurs de données, les poids d’agrégation, ainsi que le nombre d’époques et de rounds.\n",
        "## Reproductibilité : Le seed aléatoire est fixé via torch.manual_seed() pour garantir que les résultats soient reproductibles."
      ],
      "metadata": {
        "id": "GtWOkkvoJlbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNwRXgX1F-ow",
        "outputId": "ec010904-b849-473a-a526-ac1aeedb498b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model 0, Round0, loss: 2.3023083329200746\n",
            "Model 1, Round0, loss: 2.3022481083869932\n",
            "Model 0, Round1, loss: 2.295469951629639\n",
            "Model 1, Round1, loss: 2.296802508831024\n",
            "Model 0, Round2, loss: 2.291091299057007\n",
            "Model 1, Round2, loss: 2.293339455127716\n",
            "Model 0, Round3, loss: 2.2837798953056336\n",
            "Model 1, Round3, loss: 2.2879899978637694\n",
            "Model 0, Round4, loss: 2.2720150351524353\n",
            "Model 1, Round4, loss: 2.277334813276927\n",
            "Model 0, Round5, loss: 2.2526381572087604\n",
            "Model 1, Round5, loss: 2.260564299424489\n",
            "Model 0, Round6, loss: 2.2251285513242087\n",
            "Model 1, Round6, loss: 2.23793021440506\n",
            "Model 0, Round7, loss: 2.196560871601105\n",
            "Model 1, Round7, loss: 2.21383931239446\n",
            "Model 0, Round8, loss: 2.1595162908236185\n",
            "Model 1, Round8, loss: 2.189759639898936\n",
            "Model 0, Round9, loss: 2.119257962703705\n",
            "Model 1, Round9, loss: 2.1465267380078634\n"
          ]
        }
      ],
      "source": [
        "# Appel de la fonction de formation avec les modèles, les chargeurs de données, les poids d'average,\n",
        "# et les paramètres d'entraînement tels que le nombre d'époques et le nombre de rounds\n",
        "train(models, train_loaders, average_weights, epochs=epochs, rounds=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcul de l’accuracy\n",
        "### correct = 0 : Initialisation de la variable pour compter les prédictions correctes.\n",
        "### for data, target in test_loader : Boucle sur les batches du test_loader, où les données de test sont fournies.\n",
        "### output = models(data) : Applique le modèle 2 (la seconde instance du modèle) sur le batch de données.\n",
        "### pred = output.argmax(dim=1, keepdim=True) : Calcule la classe prédite en utilisant l’argmax sur les sorties du modèle."
      ],
      "metadata": {
        "id": "zfFrXbKqJxR5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3YbQgtkF-ow",
        "outputId": "5b8eb6e7-7ba7-4e33-9274-8a9fd34684a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.1089\n"
          ]
        }
      ],
      "source": [
        "# Passage des modèles en mode évaluation pour désactiver les calculs du gradient et les couches de batchnorm\n",
        "models[0].eval()  # Passe le premier modèle en mode évaluation\n",
        "models[1].eval()  # Passe le deuxième modèle en mode évaluation\n",
        "\n",
        "correct = 0  # Initialisation du compteur de prédictions correctes\n",
        "\n",
        "# Boucle sur les données du test_loader\n",
        "for data, target in test_loader:\n",
        "    # Calcul de la sortie du deuxième modèle\n",
        "    output = models[1](data)\n",
        "\n",
        "    # Prédiction avec argmax sur la sortie\n",
        "    pred = output.argmax(dim=1, keepdim=True)  # Retourne la classe ayant le plus grand score\n",
        "\n",
        "    # Calcule les prédictions correctes (comparer les prédictions avec les targets)\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()  # Compte les prédictions correctes\n",
        "\n",
        "# Affiche l'exactitude moyenne sur l'ensemble des données de test\n",
        "print(f\"Accuracy: {correct / len(test_loader.dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V43ymeQaF-ox"
      },
      "source": [
        "## Initialisation des modèles avec Seed Fixé\n",
        "### **`model1 = Net(seed=42)`**\n",
        "- **`Net(seed=42)`** : Instancie un modèle de type `Net` avec un seed spécifique fixé à 42. Cela signifie que les poids initiaux du modèle sont déterminés de manière reproductible en fonction du seed donné."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y386R_R4F-ox"
      },
      "outputs": [],
      "source": [
        "# Instanciation du premier modèle avec un seed fixe pour la reproductibilité\n",
        "model1 = Net(seed=42)  # Création du premier modèle basé sur la classe Net avec un seed défini\n",
        "\n",
        "# Instanciation du deuxième modèle avec le même seed pour assurer la reproductibilité des résultats\n",
        "model2 = Net(seed=42)  # Création du deuxième modèle basé sur la classe Net avec un seed défini\n",
        "\n",
        "# Liste contenant les deux modèles PyTorch pour les utiliser simultanément dans l'entraînement\n",
        "models = [model1, model2]  # Liste des deux modèles\n",
        "\n",
        "# Liste des chargeurs de données correspondants pour chaque sous-ensemble de données d'entraînement\n",
        "train_loaders = [train_loader1, train_loader2]  # Chargeur de données pour le premier sous-ensemble\n",
        "                                           # Chargeur de données pour le deuxième sous-ensemble\n",
        "\n",
        "# Poids utilisés pour l'average des paramètres des modèles (les deux modèles ont le même poids)\n",
        "average_weights = torch.tensor([0.5, 0.5])  # Poids uniformes pour les modèles lors de la fusion des paramètres\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exécution de la Formation des Modèles"
      ],
      "metadata": {
        "id": "BVXt5nm7KQ5l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "507xponaF-ox"
      },
      "outputs": [],
      "source": [
        "train(models, train_loaders, average_weights, epochs=epochs, rounds=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Évaluation des modèles sur les données d’entraînement\n",
        "\n",
        "- **`models[0].eval()` et `models[1].eval()`** : Met les deux modèles en mode de validation (`eval()`), où les calculs liés à la normalisation ne sont pas activés.\n",
        "- **`correct = 0`** : Initialise un compteur pour les prédictions correctes.\n",
        "- **`for data, target in train_loader1`** : Boucle sur les données d’entraînement.\n",
        "- **`output = models[1](data)`** : Applique le modèle 1 à chaque batch.\n",
        "- **`pred = output.argmax(dim=1, keepdim=True)`** : Prédictions sous forme de tensor avec la classe prédite.\n",
        "- **`pred.eq(target.view_as(pred)).sum().item()`** : Compte les prédictions correctes.\n",
        "- **`correct / len(train_loader1.dataset)`** : Calcule l’accuracy sur tout le dataset d’entraînement."
      ],
      "metadata": {
        "id": "Hq5_68UgKVRa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5ljwkl-F-ox",
        "outputId": "c738f4d0-2c9f-4ae8-dbbb-b42e20d2dc7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5333333333333333\n"
          ]
        }
      ],
      "source": [
        "# Passage des modèles en mode évaluation pour désactiver les calculs du gradient et les couches de batchnorm\n",
        "models[0].eval()  # Passe le premier modèle en mode évaluation\n",
        "models[1].eval()  # Passe le deuxième modèle en mode évaluation\n",
        "\n",
        "# Initialisation du compteur de prédictions correctes\n",
        "correct = 0\n",
        "\n",
        "# Boucle sur les données du train_loader1\n",
        "for data, target in train_loader1:\n",
        "    # Calcul de la sortie du deuxième modèle\n",
        "    output = models[1](data)\n",
        "\n",
        "    # Prédiction avec argmax sur la sortie\n",
        "    pred = output.argmax(dim=1, keepdim=True)  # Retourne la classe ayant le plus grand score\n",
        "\n",
        "    # Calcule les prédictions correctes (comparer les prédictions avec les targets)\n",
        "    correct += pred.eq(target.view_as(pred)).sum().item()  # Compte les prédictions correctes\n",
        "\n",
        "# Affiche l'exactitude moyenne sur l'ensemble des données d'entraînement\n",
        "print(f\"Accuracy: {correct / len(train_loader1.dataset)}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bio",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}